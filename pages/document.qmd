---
title-block-banner: white
title-block-banner-color: black
title: A Sample Quarto Document
author: Rohan Sikand
citation: true
date: 10/20/2020
toc: false
format: 
  html: default
  pdf: default
  docx: default
---

The following is a sample Quarto document which highlights several autoring features. This page was mainly created for my own reference. I use a recent lecture note as a base to edit off of.  

## Cool features

(Test)... here is a theorem: 

:::  {.callout-note appearance="simple"}

::: {#thm-line}

## Line

The equation of any straight line, called a linear equation, can be written as:

$$
y = mx + b
$$
:::
:::


## Topics 

- Modern DL methods for learning across tasks 
- Implementing these methods (MT, TL) in PyTorch 
- Glimpse of building new algorithms 

low-level descriptions: 

- MT, TL
- Meta learning algos 
- Advanced meta learning topics
- Unsupervised pre-training
    - FS learning
- Domain adaption 
- Lifelong learning 
- Open problems

Focus on DL, with case studies in things like NLP. 
- No RL! (see CS 224R) 

## 1. Logistics 
- Lectures are live-streamed and recorded 
- two guest lectures
- Prereqs:
    - Sufficient background in ML (229) 

### Homeworks 
50% of grade. 

- 0: multi-task basics
- 1: multi-task data processing and BB-ML
- 2: gradient-based ML 
- 3: fine-tuning pre-trained language models 
- 4 (optional): Bayesian ML and meta overfitting 
    - Replace 15% of hw/project 
    - Not coding, all math 
- 6 late days 

### Project

Here is a footnote reference,[^1] and another.[^longnote]

[^1]: Here is the footnote.

[^longnote]: Here's one with multiple blocks.

    Subsequent paragraphs are indented to show that they
belong to the previous footnote.

        { some.code }

    The whole paragraph can be indented, or just the first
    line.  In this way, multi-paragraph footnotes work like
    multi-paragraph list items.

This paragraph won't be part of the note, because it
isn't indented.

Here is a bib citation. Blah Blah [see @knuth1984, pp. 33-35;
also @wickham2015, chap. 1]


- Poster session, 50% of grade. 
- Idea: ... 

Now technical content... 

## 2. Why study multi-task learning and meta-learning?
- How can we enable agents to learn a breadth of skills in the real world? 
    - Because each time we have to train a supervised signal 
        - So the goal is to learn representations across tasks 
- Aside (common paradigm to learn representations): initialize well (not randomly) --> fine-tune on new task. 
    - This is harder for RL than NLP because NLP has the entire wikipedia to use but robotic common sense representations are not as straightforward (maybe we need a common robot embedding?)

**Evolution**:

- Early in CV: hand-design features, train SVM on-top 
- Modern CV: end-to-end training, no hand-engineering
    - Allows us to handle unstructured inputs without understanding it
- Now why meta-learning? Three reasons... 
    - **Don't have large dataset** at the outset to pre-train on or use in end-to-end SL manner (med imaging, robotics, etc.)
        - Even more so: **long-tail data** samples (e.g., self-driving won't catch all edge cases)
            - MEL techniques can help with this (kinda... not the main focus tho)
    - **Quickly learn something new** (few-shot learning)
- Lots of open problems 

## Multi-task intro 

Some `code` block: 

```python
print(5)
```


- What is a $t$ task? See @thm-line.
    - Dataset + loss objective --> model 
    - Objects as "tasks" 
    - Critical assumption: different tasks need to share some base structure (goal is to exploit shared structure)
        - But lots of tasks share structure (even as upstream as sharing the laws of physics!)
        - Question: can we learn a shared embedding space for e.g., text + images in one? 
- Does MT learning reduce to single-task SL learning?
    - Somewhat (tho not for every problem)
    - Idea: sum loss and data: 
    
$$
\mathcal{D}=\bigcup \mathcal{D}_i \quad \mathcal{L}=\sum \mathcal{L}_i
$$




Next up: a technical dive into the **multi-task** learning framework. 

<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images/images/c49969eb11cb825d222502ce7dba1133f0ab193896de5d7a97599632a20c990c.png" width="300" />  
</p>


## Acknowledgments {.appendix}

I am grateful for the insightful comments offered by the anonymous peer reviewers at Books & Texts. The generosity and expertise of one and all have improved this study in innumerable ways and saved me from many errors; those that inevitably remain are entirely my own responsibility.