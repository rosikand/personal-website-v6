[
  {
    "objectID": "zebras/one/m.html",
    "href": "zebras/one/m.html",
    "title": "Even more text",
    "section": "",
    "text": "For the past year and a half, I have been experimenting with my digital note-taking setup, ranging from live typesetting LaTeX to Apple Notes. I don‚Äôt think I can find a perfect solution that meets all my needs. But I think we can get close. I plan to make a bigger post describing my setup in more detail (in a similar fashion to castel.dev), but one of the main components is what we discuss here: how to use VS Code to incorporate images into your Markdown documents. We use Markdown because it supports things like embedded LaTeX math and code fences (such features are pivotal when taking technical notes during class)."
  },
  {
    "objectID": "zebras/one/mm.html",
    "href": "zebras/one/mm.html",
    "title": "Even more text",
    "section": "",
    "text": "For the past year and a half, I have been experimenting with my digital note-taking setup, ranging from live typesetting LaTeX to Apple Notes. I don‚Äôt think I can find a perfect solution that meets all my needs. But I think we can get close. I plan to make a bigger post describing my setup in more detail (in a similar fashion to castel.dev), but one of the main components is what we discuss here: how to use VS Code to incorporate images into your Markdown documents. We use Markdown because it supports things like embedded LaTeX math and code fences (such features are pivotal when taking technical notes during class)."
  },
  {
    "objectID": "zebras/two/mmm.html",
    "href": "zebras/two/mmm.html",
    "title": "Even more text",
    "section": "",
    "text": "For the past year and a half, I have been experimenting with my digital note-taking setup, ranging from live typesetting LaTeX to Apple Notes. I don‚Äôt think I can find a perfect solution that meets all my needs. But I think we can get close. I plan to make a bigger post describing my setup in more detail (in a similar fashion to castel.dev), but one of the main components is what we discuss here: how to use VS Code to incorporate images into your Markdown documents. We use Markdown because it supports things like embedded LaTeX math and code fences (such features are pivotal when taking technical notes during class)."
  },
  {
    "objectID": "port.html",
    "href": "port.html",
    "title": "Portfolio",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "port.html#projects",
    "href": "port.html#projects",
    "title": "Portfolio",
    "section": "üõ∂ projects",
    "text": "üõ∂ projects\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 30, 2022\n\n\nEven more text\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "port.html#writing",
    "href": "port.html#writing",
    "title": "Portfolio",
    "section": "üìù Writing",
    "text": "üìù Writing\n\n\n\n\nYou can also view some more (older) projects here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rohan Sikand",
    "section": "",
    "text": "Hey, I‚Äôm Rohan! I‚Äôm a student at The Farm studying Computer Science with interests in software development, machine learning, computer systems, and applied mathematics. This is my personal home page where I keep a portfolio of my work, among other things (such as a blog). Most my work is on my Github. In my free time, I enjoy playing golf."
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "B.S. Computer Science ‚Ä¢ 2020 ‚Äì 2024 ‚Ä¢ Stanford, CA\n\nSelected coursework: CS 221 (AI, in progress), CS 330 (Meta Learning, in progress), CS 107 (Computer Systems), ENGR 108 (Matrix Methods), CS 109 (Probability), CS 111 (Operating Systems), CS 279 (Computational Biology), CS 103 (Theory of Computing), Math 51 (Linear Algebra + Multivariable Calculus). See here for more."
  },
  {
    "objectID": "experience.html#experience",
    "href": "experience.html#experience",
    "title": "Experience",
    "section": "Experience",
    "text": "Experience\n\n Machine Learning Engineering Intern at insitro\nJune 2022 ‚Äì September 2022 ‚Ä¢ South San Francisco, CA\n\nConducted cross-disciplinary neuron segmentation project using machine learning for the purposes of flourescent phenotyping and disease modeling. Deployed models with an API. Used ViT, DINO, Segformer, DeepLabV3, etc.\nGained experience working with industry tools such as Amazon AWS EC2 & Batch, Weights & Biases.\nSee here for more details."
  },
  {
    "objectID": "blog/markdown-image-workflow.html",
    "href": "blog/markdown-image-workflow.html",
    "title": "A Quick Workflow for Incorporating Images into your Markdown Documents using VS Code",
    "section": "",
    "text": "For the past year and a half, I have been experimenting with my digital note-taking setup, ranging from live typesetting LaTeX to Apple Notes. I don‚Äôt think I can find a perfect solution that meets all my needs. But I think we can get close. I plan to make a bigger post describing my setup in more detail (in a similar fashion to castel.dev), but one of the main components is what we discuss here: how to use VS Code to incorporate images into your Markdown documents. We use Markdown because it supports things like embedded LaTeX math and code fences (such features are pivotal when taking technical notes during class)."
  },
  {
    "objectID": "blog/markdown-image-workflow.html#the-standard-workflow",
    "href": "blog/markdown-image-workflow.html#the-standard-workflow",
    "title": "A Quick Workflow for Incorporating Images into your Markdown Documents using VS Code",
    "section": "The standard workflow",
    "text": "The standard workflow\nIncorporating images is a necessary function to have when typing notes, live, during lecture. And not only that, we need to do this fast. This is problematic in plain markdown because it requires us to‚Ä¶\n\nDownload the image file and save it to some path\nSpecify the path relative to your document like so: ![alt text](../some_long/path/im.png) in the document itself.\n\nIn some ways, you‚Äôd be done there. But‚Ä¶ most figures are quite large and take up majority of the page‚Ä¶\n\n\n\nno good! And guess what? There is no standard way to resize images in markdown! So you‚Äôd have to specify an HTML element like so:\n&lt;img src=\"http://www.fillmurray.com/460/300\"&gt;\nFrom here, you could in theory add image alignment and sizing, but this still takes another ~45 seconds:\n&lt;p align=\"center\"&gt;\n    &lt;img alt=\"picture 1\" src=\"http://www.fillmurray.com/460/300\" width=\"350\" /&gt;  \n&lt;/p&gt;\nIn all, this whole process probably takes ~2 minutes per image. And if you are typing live during a lecture, you‚Äôll quickly be lost in the content and won‚Äôt be able to keep up with the lecturer. For these reasons, after playing around with various ideas, I describe a new workflow below."
  },
  {
    "objectID": "blog/markdown-image-workflow.html#a-new-efficient-workflow",
    "href": "blog/markdown-image-workflow.html#a-new-efficient-workflow",
    "title": "A Quick Workflow for Incorporating Images into your Markdown Documents using VS Code",
    "section": "A new, efficient workflow",
    "text": "A new, efficient workflow\nFirst of all, this workflow is powered by:\n\nVS Code (necessary to use due to the use of the extension listed below)\nMarkdown Image VS Code extension by Hancel.Lin (link)\n\nThe main philosophy:\n\nWe want to upload the images on a remote server and retrieve a URL in return. We will use this as the path since it eliminates the need to download, save, and specify the path locally.\n\nWe use a public Github repo to upload the images to automatically. This allows us to store our images on Github‚Äôs servers and retrieve a URL on the internet.\n\nNote: You can also use this setup with some other CDN delivery network that the Markdown Image extension allows for (such as Cloudflare). But I personally think Github is the safest and best option.\n\n\n\n\nSetup\n\nMake a new Github repo to store the images.\nOpen up the config settings for the above extension in VS Code. Specify the following fields based on your account info and repo:\n\nbranch\npath (i.e., the folder inside your GH repo where you want to store the images)\nrepository name\ntoken (your personal access token)\n\nNow change the base upload method field to ‚ÄòGithub‚Äô in the extension settings.\n(Optional) while you are at it, you can also change the base image width in the extension settings, the code type (I prefer the HTML string over raw Markdown since it renders nicely in Github‚Äôs web interface), the max image width (I use 300-400).\n(Optional) if you want center-alignment, make a VS Code snippet with the following code:\n\n&lt;p align=\"center\"&gt;\n     \n&lt;/p&gt;\nand then paste your image in between the element block dividers.\nAnd that‚Äôs it! All you need to do is copy an image to your clipboard (i.e., from a screenshot) and paste it in using the extension by right-clicking and select (if enabled and there is other extensions overwriting the command):\n\n\n\ndone! Now your image uploading should take about ~5 seconds (a much better improvement from ~2 minutes!)."
  },
  {
    "objectID": "blog/functorch-example.html",
    "href": "blog/functorch-example.html",
    "title": "A Simple functorch Example",
    "section": "",
    "text": "In recent years, there has been a small movement of people trying to go from stateful (Python OOP, class-based modules) to stateless (pure functions) neural network code. The standard PyTorch nn.module is indeed a OOP-based class. But more recent libraries such as JAX, introduce the ability to feasiby create stateless (just functions!) machine learning models. Now, in PyTorch version 1.13, we have functorch in-tree (in the main package). Why stateless? Read this blog post for the differences, but some reasons for why I like stateless code is because:\n\nLess leaky abstractions (and less unknown abstractions in general!)\nCloser to the mathematical form (after all, a neural network is just a series of functions chained together!)\n\nWhen you learn SGD in class in the mathematical form and then use PyTorch, the disconnect is fairly evident.\n\nLess compute overhead (less things to keep track of internally ‚Äì&gt; less memory needed)\nAbility to work a lower level (which, in my opinion, can help facilitate new ideas)\nAbility to work with function transformations such as vmap, pmap, jit, and grad (PyTorch has grad‚Ä¶ yes I know‚Ä¶ but applying grad to a stateless function makes much more intuitive sense than applying it to some stateful module!).\n\nThis might sound like an advertisement for JAX (which might be coming up in a future blog post!), but it is really to set the stage for functorch. functorch is a library that allows you to accomplish nearly all of the above, but in PyTorch! The basic idea is to purify stateful PyTorch modules into stateless functions like this (source):\nimport torch\nimport functorch\nfrom functorch import make_functional\n\nmodel = torch.nn.Linear(3, 3)\nfunc_model, params = make_functional(model)\nAs functorch is relatively new, there aren‚Äôt many examples out there showing how to use the library. So the goal of the rest of this post is to provide a simple example for creating an image classifier using functorch and PyTorch and updating the weights using SGD (no torch.optim!).\nHere is the code:\nimport torch\nimport torchplate\nfrom torchplate import experiment\nfrom torchplate import utils\nimport functorch\nfrom functorch import grad, grad_and_value\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom rsbox import ml\n# import torchopt\nimport requests\nfrom tqdm.auto import tqdm\nimport cloudpickle as cp\nfrom urllib.request import urlopen\n\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(3*32*32, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 16)\n        self.fc4 = nn.Linear(16, 3)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\n\n\nclass OptExp:\n    def __init__(self): \n        self.model_module = Net()\n        self.criterion = nn.CrossEntropyLoss()\n        dataset = cp.load(urlopen(\"https://stanford.edu/~rsikand/assets/datasets/mini_cifar.pkl\")) \n        self.trainloader, self.testloader = torchplate.utils.get_xy_loaders(dataset)\n        self.model, self.params = functorch.make_functional(self.model_module)  # init network \n    \n\n    def predict(self, x):\n        \"\"\"returns logits\"\"\"\n        assert self.model is not None\n        assert self.params is not None\n        logits = self.model(self.params, x)\n        return logits \n\n\n    @staticmethod\n    def sgd_step(params, gradients, lr):\n        \"\"\"one gradient step for updating the weights\"\"\"\n        updated_params = []\n        for param, gradient in zip(params, gradients):\n            update = param - (lr * gradient)\n            updated_params.append(update)\n        \n        return tuple(updated_params)\n    \n\n    @staticmethod\n    def stateless_loss(params, model, criterion, batch):\n        \"\"\"\n        Need to perform forward pass and loss calculation in one function\n        since we need gradients w.r.t params (must be args[0]). The first\n        value we return also needs to be the scalar loss value.  \n        \"\"\"\n        x, y = batch\n        logits = model(params, x)\n        loss_val = criterion(logits, y)\n        return loss_val, logits\n    \n\n\n    @staticmethod\n    def train_step(params, model, criterion, batch, lr):\n        \"\"\"Combine this all into one function for modularity\"\"\"\n        # has_aux means we can return more than just the scalar loss \n        grad_and_loss_fn = grad_and_value(OptExp.stateless_loss, has_aux=True)  \n        grads, aux_outputs = grad_and_loss_fn(params, model, criterion, batch)  # get the grads \n        loss_val, logits = aux_outputs\n        params = OptExp.sgd_step(params, grads, lr) \n        return params, loss_val, logits\n\n    \n    def train(self, num_epochs=10, lr=0.01):\n        print('Beginning training!')\n        epoch_num = 0\n        for epoch in range(num_epochs):\n            running_loss = 0.0\n            epoch_num += 1\n            tqdm_loader = tqdm(self.trainloader)\n            for batch in tqdm_loader:\n                tqdm_loader.set_description(f\"Epoch {epoch_num}\")\n\n                # update params with one step \n                self.params, loss_val, logits = OptExp.train_step(self.params, self.model, self.criterion, batch, lr)\n\n                running_loss += loss_val\n\n            # print loss\n            epoch_avg_loss = running_loss/len(self.trainloader)\n            print(\"Training Loss (epoch \" + str(epoch_num) + \"):\", epoch_avg_loss)\n\n\n        print('Finished training!')\n\n\nexp = OptExp()\nexp.train(num_epochs=50, lr=0.01)"
  },
  {
    "objectID": "blog/archived/wow.html",
    "href": "blog/archived/wow.html",
    "title": "Some quarto notes",
    "section": "",
    "text": "Red\nGreen\nBlue"
  },
  {
    "objectID": "blog/archived/wow.html#shapes",
    "href": "blog/archived/wow.html#shapes",
    "title": "Some quarto notes",
    "section": "Shapes",
    "text": "Shapes\n\nSquare\nCircle\nTriangle"
  },
  {
    "objectID": "blog/archived/wow.html#textures",
    "href": "blog/archived/wow.html#textures",
    "title": "Some quarto notes",
    "section": "Textures",
    "text": "Textures\n\nSmooth\nBumpy\nFuzzy"
  },
  {
    "objectID": "blog/archived/wow.html#physics",
    "href": "blog/archived/wow.html#physics",
    "title": "Some quarto notes",
    "section": "Physics",
    "text": "Physics\nEinstein‚Äôs theory of special relatively that expresses the equivalence of mass and energy: E = mc^{2}. See Equation¬†1 to better understand standard deviation.\nbetter understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard better understand standard\nEven more so:\n\n4x + 5 = 6\n\\tag{1}\n\nLike wow!wmqwldqwwe\n\nAnd wow! Even add callout:\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important. Some math 4 involved: \n4x+3=2\n\\\\\n4x=-1\n\\\\\nx=\\frac{-1}{4}\n\nAnd some code:\nprint(5)\n\n\nOh how cool."
  },
  {
    "objectID": "blog/archived/wow.html#rendering-code",
    "href": "blog/archived/wow.html#rendering-code",
    "title": "Some quarto notes",
    "section": "Rendering code",
    "text": "Rendering code\n\ns\n\n\nThis is awspan that has the class aside which places it in the margin without a footnote number.wljekw\nns qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. sqwdl,mqwqwdl,mqw qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk. s qmnmwne,mqw qwkjdklqwkdqk."
  },
  {
    "objectID": "blog/ai-for-climate-change.html",
    "href": "blog/ai-for-climate-change.html",
    "title": "Applications of AI for Climate Change",
    "section": "",
    "text": "Name\nTL;DR Description\nLinks\n\n\n\n\nSeagrass Monitoring\nMeasure and monitor biomass on seabeds remotely. Hydrography.\n[1]\n\n\nUnited Nations Sustainable Development Goals\n17 goals provided by the United Nations for sustainable development. There exists many potential applications of AI for some of the goals.\n[1]\n\n\n\nShort list:\n\nhttps://www.drivendata.org/competitions/143/tick-tick-bloom/\nhttps://www.drivendata.org/competitions/99/biomass-estimation/\nhttps://www.drivendata.org/competitions/group/competition-reclamation-snow-water/\nhttps://www.drivendata.org/competitions/81/detect-flood-water/\nhttps://www.drivendata.org/competitions/83/cloud-cover/\nhttps://sustainlab-group.github.io/sustainbench/about/"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "My research interests include deep learning and its applications to fields such as computer vision and medicine.\n\n\n\n\n\n\n\n\n\n\n\ninsitro internship project: Conducted cross-disciplinary neuron segmentation project using machine learning for the purposes of flourescent phenotyping and disease modeling. Deployed models with an API. Used ViT, DINO, Segformer, DeepLabV3, etc. Gained experience working with industry tools such as Amazon AWS EC2 & Batch, Weights & Biases.  Skills: PyTorch, Jax, computer vision, neural networks, Hugging Face, albumentations, AWS Batch and EC2, Weights and Biases\n\n\n\n\n\n\n\n\n\n\n\n\nParticipated in the Canary CREST research program at Stanford Medicine (Urologic Cancer Innovation Lab). There, I conducted a research project involving the registration of the T2 and DWI b1200 MRI Sequences of the Prostate using classical and deep learning methods.  Skills: PyTorch, computer vision, deformable fields, neural networks, affine transformations  [paper] [poster] [code]\n\n\n\n\n\n\n\n\n\n\n\n\nApplying ML concepts to projects with peers.\n\n\n‚ÄúTeam Stanford ACMLab at SemEval 2022 Task 4: Textual Analysis of PCL Using Contextual Word Embeddings‚Äù (paper)\n\n\nDeveloped benchmark tests for large language models: ‚ÄùLogic grid puzzles‚Äù RG, WZ, RS, JK, EC, JT (2021). WELM@ICLR (Spotlight). Published in ‚ÄúBeyond the Imitation Game: Quantifying and extrapolating the capabilities of language models‚Äù (arXiv).\n\n\n‚ÄùMapping Income Distribution with Machine Learning‚Äù (link).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuring my first year at Stanford, I was a research intern in the Mignot Lab at Stanford School of Medicine. I conducted a project involving the use of sequence-based deep learning architectures, such as Long short-term memory networks, to classify sleep sounds via mel spectogram representations of audio waves.  Skills: Keras, Tensorflow, Librosa, neural networks  [code]"
  },
  {
    "objectID": "portfolio.html#projects",
    "href": "portfolio.html#projects",
    "title": "Portfolio",
    "section": "üõ∂ Projects",
    "text": "üõ∂ Projects\n\n\n\n\n   \n\n\n\n\n\n\n\nTorchplate\nA minimal experiment framework and boilerplate for machine learning workflows in PyTorch. Provide evaluate() and a few other ML experiment specific things and train (exp.train(num_epochs=100)) away!  Installation: $ pip install torchplate.\n\n\n\n\n\n\n\n\n\n\n\nrsbox\nAn open source toolbox of utility functions for common workflows I use in Python (mostly machine learning things).\nInstallation: $ pip install rsbox.\n\n\n\n\n\n\n\n\n\n\n\nLemnos\nAn open source light-weight command-line to-do list manager built with Python.\n\n\n\n\n\n\n\nHere are some other projects.\n\nüåÄ Probabilistic Methods for Diagnosing Parkinson‚Äôs Disease in Hand-Drawn Spirals: Modeled curvature in hand-drawn spirals as Gaussian distributions for inference: a heuristic for diagnosing Parkinson‚Äôs disease (CS 109 challenge winner, 1/239).\nüß´ CS 279 Project: Deep Learning for Classifying Subcellular Patterns of Proteins in Microscopic Images.\nüöÇ easyset: Easy-to-use, small, pre-processed datasets in Python for machine learning (work-in-progress).\nü™É markdown2laTeX: Program that produces a LaTeX output based on markdown source as input."
  },
  {
    "objectID": "portfolio.html#writing-and-exposition",
    "href": "portfolio.html#writing-and-exposition",
    "title": "Portfolio",
    "section": "üìù Writing and Exposition",
    "text": "üìù Writing and Exposition\nI like to take long and short form notes on concepts that I am learning about.\n\nüîå snippets: Toolbox of short, reusable pieces of code and knowledge.\nüìã PyTorch reference document\nüôã‚Äç‚ôÇÔ∏è Deep Learning Presentation: talk I gave back in high school for my multivariable calculus class.\n\nStanford course notes coming soon! (see some old ones here!)"
  },
  {
    "objectID": "archived/research.html",
    "href": "archived/research.html",
    "title": "Research",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "archived/projects.html",
    "href": "archived/projects.html",
    "title": "Projects",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pages/document.html",
    "href": "pages/document.html",
    "title": "A Sample Quarto Document",
    "section": "",
    "text": "The following is a sample Quarto document which highlights several autoring features. This page was mainly created for my own reference. I use a recent lecture note as a base to edit off of."
  },
  {
    "objectID": "pages/document.html#cool-features",
    "href": "pages/document.html#cool-features",
    "title": "A Sample Quarto Document",
    "section": "Cool features",
    "text": "Cool features\n(Test)‚Ä¶ here is a theorem:\n\n\n\n\n\n\n\nTheorem 1 (Line) The equation of any straight line, called a linear equation, can be written as:\n\\[\ny = mx + b\n\\]"
  },
  {
    "objectID": "pages/document.html#topics",
    "href": "pages/document.html#topics",
    "title": "A Sample Quarto Document",
    "section": "Topics",
    "text": "Topics\n\nModern DL methods for learning across tasks\nImplementing these methods (MT, TL) in PyTorch\nGlimpse of building new algorithms\n\nlow-level descriptions:\n\nMT, TL\nMeta learning algos\nAdvanced meta learning topics\nUnsupervised pre-training\n\nFS learning\n\nDomain adaption\nLifelong learning\nOpen problems\n\nFocus on DL, with case studies in things like NLP. - No RL! (see CS 224R)"
  },
  {
    "objectID": "pages/document.html#logistics",
    "href": "pages/document.html#logistics",
    "title": "A Sample Quarto Document",
    "section": "1. Logistics",
    "text": "1. Logistics\n\nLectures are live-streamed and recorded\ntwo guest lectures\nPrereqs:\n\nSufficient background in ML (229)\n\n\n\nHomeworks\n50% of grade.\n\n0: multi-task basics\n1: multi-task data processing and BB-ML\n2: gradient-based ML\n3: fine-tuning pre-trained language models\n4 (optional): Bayesian ML and meta overfitting\n\nReplace 15% of hw/project\nNot coding, all math\n\n6 late days\n\n\n\nProject\nHere is a footnote reference,1 and another.2\nThis paragraph won‚Äôt be part of the note, because it isn‚Äôt indented.\nHere is a bib citation. Blah Blah [see @knuth1984, pp. 33-35; also @wickham2015, chap. 1]\n\nPoster session, 50% of grade.\nIdea: ‚Ä¶\n\nNow technical content‚Ä¶"
  },
  {
    "objectID": "pages/document.html#why-study-multi-task-learning-and-meta-learning",
    "href": "pages/document.html#why-study-multi-task-learning-and-meta-learning",
    "title": "A Sample Quarto Document",
    "section": "2. Why study multi-task learning and meta-learning?",
    "text": "2. Why study multi-task learning and meta-learning?\n\nHow can we enable agents to learn a breadth of skills in the real world?\n\nBecause each time we have to train a supervised signal\n\nSo the goal is to learn representations across tasks\n\n\nAside (common paradigm to learn representations): initialize well (not randomly) ‚Äì> fine-tune on new task.\n\nThis is harder for RL than NLP because NLP has the entire wikipedia to use but robotic common sense representations are not as straightforward (maybe we need a common robot embedding?)\n\n\nEvolution:\n\nEarly in CV: hand-design features, train SVM on-top\nModern CV: end-to-end training, no hand-engineering\n\nAllows us to handle unstructured inputs without understanding it\n\nNow why meta-learning? Three reasons‚Ä¶\n\nDon‚Äôt have large dataset at the outset to pre-train on or use in end-to-end SL manner (med imaging, robotics, etc.)\n\nEven more so: long-tail data samples (e.g., self-driving won‚Äôt catch all edge cases)\n\nMEL techniques can help with this (kinda‚Ä¶ not the main focus tho)\n\n\nQuickly learn something new (few-shot learning)\n\nLots of open problems"
  },
  {
    "objectID": "pages/document.html#multi-task-intro",
    "href": "pages/document.html#multi-task-intro",
    "title": "A Sample Quarto Document",
    "section": "Multi-task intro",
    "text": "Multi-task intro\nSome code block:\nprint(5)\n\nWhat is a \\(t\\) task? See Theorem¬†1.\n\nDataset + loss objective ‚Äì> model\nObjects as ‚Äútasks‚Äù\nCritical assumption: different tasks need to share some base structure (goal is to exploit shared structure)\n\nBut lots of tasks share structure (even as upstream as sharing the laws of physics!)\nQuestion: can we learn a shared embedding space for e.g., text + images in one?\n\n\nDoes MT learning reduce to single-task SL learning?\n\nSomewhat (tho not for every problem)\nIdea: sum loss and data:\n\n\n\\[\n\\mathcal{D}=\\bigcup \\mathcal{D}_i \\quad \\mathcal{L}=\\sum \\mathcal{L}_i\n\\]\nNext up: a technical dive into the multi-task learning framework."
  },
  {
    "objectID": "pages/document.html#acknowledgments",
    "href": "pages/document.html#acknowledgments",
    "title": "A Sample Quarto Document",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nI am grateful for the insightful comments offered by the anonymous peer reviewers at Books & Texts. The generosity and expertise of one and all have improved this study in innumerable ways and saved me from many errors; those that inevitably remain are entirely my own responsibility."
  },
  {
    "objectID": "pages/insitro.html",
    "href": "pages/insitro.html",
    "title": "Insitro Internship - Summer 2022",
    "section": "",
    "text": "Machine Learning Engineering Intern - insitro, Summer 2022"
  },
  {
    "objectID": "pages/insitro.html#experience",
    "href": "pages/insitro.html#experience",
    "title": "Insitro Internship - Summer 2022",
    "section": "Experience",
    "text": "Experience\nIn the Summer of 2022, I interned at insitro, an ML-powered drug discovery company, led by Daphne Koller. My experience was nothing short of amazing, and I definately learned a ton. While I had a lot of machine learning engineering experience prior to coming to insitro, I lacked knowledge of industry tools. At insitro, I was able to broaden my horizons and learn about concepts such as Amazon AWS, EC2, Batch; Weights and Biases; Git and Github pull requests; PyTorch distributed; and much more."
  },
  {
    "objectID": "pages/insitro.html#project",
    "href": "pages/insitro.html#project",
    "title": "Insitro Internship - Summer 2022",
    "section": "Project",
    "text": "Project\nIn this section, I give a brief overview of the project I conducted during my time at insitro.\n\nProject title: ‚ÄúExploring Experimental Machine Learning Strategies for Improving Neuron Segmentation Models‚Äù"
  },
  {
    "objectID": "pages/coursework.html",
    "href": "pages/coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "INTLPOL 268: Hack Lab: Introduction to Cybersecurity\nCS 221: Artificial Intelligence: Principles and Techniques\nCS 330: Deep Multi-task and Meta Learning\nBIO 3: Frontiers in Marine Biology\nPEDS 220: Covid-19 Elective\nCS 64: Computation for Puzzles and Games\n\n\n\n\n\nCS 111: Operating Systems Principles\nCS 529: Robotics and Autonomous Systems Seminar\nBIO 150: Human Behavioral Biology\nBIOE 131: Ethics in Bioengineering\nEE 292I: Insanely Great Products: How do they get built?\n\n\n\n\n\nCS 109: Introduction to Probability for Computer Scientists\nMath 51: Linear Algebra, Multivariable Calculus, and Modern Applications\nPWR 2SPB: Hope, Health, and Healing: The Rhetoric of Medicine\nCS 22A: The Social & Economic Impact of Artificial Intelligence\n\n\n\n\n\nCS 107: Computer Organization and Systems\nENGR 108: Introduction to Matrix Methods\nCS 279: Computational Biology: Structure and Organization of Biomolecules and Cells\nCS 528: Machine Learning Systems Seminar\nMS&E 472: Entrepreneurial Thought Leaders‚Äô Seminar\nPEDS 220: Covid-19 Elective\n\n\n\n\n\nCS 103: Mathematical Foundations of Computing\nBIOE 70Q: Medical Device Innovation\nECON 23N: Capitalism, Socialism and Democracy\nTHINK 23: The Cancer Problem\nCS 41: Hap.py Code: The Python Programming Language Winter 2021\n\n\n\n\n\nCS 106B: Programming Abstractions\nPWR 1CK: Investigating the News: Journalism, Technology & the Future\nPSYC 199: Undergraduate Research\nMS&E 472: Entrepreneurial Thought Leaders‚Äô Seminar\n\n\n\n\n\nSYMSYS 1: Minds and Machines\nCS 106A: Programming Methodology\nMath 21: Calculus II\nMATSCI 83N: Great Inventions that Matter\nCS 523: Seminar in Artificial Intelligence in Healthcare\nEDUC 157: Election 2020"
  },
  {
    "objectID": "pages/coursework.html#high-school",
    "href": "pages/coursework.html#high-school",
    "title": "Coursework",
    "section": "High School",
    "text": "High School\nSome relevant coursework from high school:\n\nMultivariable Calculus (12th)\nHonors Computer Science (10th)\nIntroduction to Data Structures and Algorithms (12th)\nAP Computer Science A (11th)\nAP Computer Science Principles (11th)\nAP Calculus (11th)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "A Quick Workflow for Incorporating Images into your Markdown Documents using VS Code\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nA Simple functorch Example\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nApplications of AI for Climate Change\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html#research",
    "href": "portfolio.html#research",
    "title": "Portfolio",
    "section": "",
    "text": "My research interests include deep learning and its applications to fields such as computer vision and medicine.\n\n\n\n\n\n\n\n\n\n\n\ninsitro internship project: Conducted cross-disciplinary neuron segmentation project using machine learning for the purposes of flourescent phenotyping and disease modeling. Deployed models with an API. Used ViT, DINO, Segformer, DeepLabV3, etc. Gained experience working with industry tools such as Amazon AWS EC2 & Batch, Weights & Biases.  Skills: PyTorch, Jax, computer vision, neural networks, Hugging Face, albumentations, AWS Batch and EC2, Weights and Biases\n\n\n\n\n\n\n\n\n\n\n\n\nParticipated in the Canary CREST research program at Stanford Medicine (Urologic Cancer Innovation Lab). There, I conducted a research project involving the registration of the T2 and DWI b1200 MRI Sequences of the Prostate using classical and deep learning methods.  Skills: PyTorch, computer vision, deformable fields, neural networks, affine transformations  [paper] [poster] [code]\n\n\n\n\n\n\n\n\n\n\n\n\nApplying ML concepts to projects with peers.\n\n\n‚ÄúTeam Stanford ACMLab at SemEval 2022 Task 4: Textual Analysis of PCL Using Contextual Word Embeddings‚Äù (paper)\n\n\nDeveloped benchmark tests for large language models: ‚ÄùLogic grid puzzles‚Äù RG, WZ, RS, JK, EC, JT (2021). WELM@ICLR (Spotlight). Published in ‚ÄúBeyond the Imitation Game: Quantifying and extrapolating the capabilities of language models‚Äù (arXiv).\n\n\n‚ÄùMapping Income Distribution with Machine Learning‚Äù (link).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuring my first year at Stanford, I was a research intern in the Mignot Lab at Stanford School of Medicine. I conducted a project involving the use of sequence-based deep learning architectures, such as Long short-term memory networks, to classify sleep sounds via mel spectogram representations of audio waves.  Skills: Keras, Tensorflow, Librosa, neural networks  [code]"
  },
  {
    "objectID": "experience.html#education",
    "href": "experience.html#education",
    "title": "Experience",
    "section": "",
    "text": "B.S. Computer Science ‚Ä¢ 2020 ‚Äì 2024 ‚Ä¢ Stanford, CA\n\nSelected coursework: CS 221 (AI, in progress), CS 330 (Meta Learning, in progress), CS 107 (Computer Systems), ENGR 108 (Matrix Methods), CS 109 (Probability), CS 111 (Operating Systems), CS 279 (Computational Biology), CS 103 (Theory of Computing), Math 51 (Linear Algebra + Multivariable Calculus). See here for more."
  },
  {
    "objectID": "pages/coursework.html#stanford-university-b.s.-computer-science",
    "href": "pages/coursework.html#stanford-university-b.s.-computer-science",
    "title": "Coursework",
    "section": "",
    "text": "INTLPOL 268: Hack Lab: Introduction to Cybersecurity\nCS 221: Artificial Intelligence: Principles and Techniques\nCS 330: Deep Multi-task and Meta Learning\nBIO 3: Frontiers in Marine Biology\nPEDS 220: Covid-19 Elective\nCS 64: Computation for Puzzles and Games\n\n\n\n\n\nCS 111: Operating Systems Principles\nCS 529: Robotics and Autonomous Systems Seminar\nBIO 150: Human Behavioral Biology\nBIOE 131: Ethics in Bioengineering\nEE 292I: Insanely Great Products: How do they get built?\n\n\n\n\n\nCS 109: Introduction to Probability for Computer Scientists\nMath 51: Linear Algebra, Multivariable Calculus, and Modern Applications\nPWR 2SPB: Hope, Health, and Healing: The Rhetoric of Medicine\nCS 22A: The Social & Economic Impact of Artificial Intelligence\n\n\n\n\n\nCS 107: Computer Organization and Systems\nENGR 108: Introduction to Matrix Methods\nCS 279: Computational Biology: Structure and Organization of Biomolecules and Cells\nCS 528: Machine Learning Systems Seminar\nMS&E 472: Entrepreneurial Thought Leaders‚Äô Seminar\nPEDS 220: Covid-19 Elective\n\n\n\n\n\nCS 103: Mathematical Foundations of Computing\nBIOE 70Q: Medical Device Innovation\nECON 23N: Capitalism, Socialism and Democracy\nTHINK 23: The Cancer Problem\nCS 41: Hap.py Code: The Python Programming Language Winter 2021\n\n\n\n\n\nCS 106B: Programming Abstractions\nPWR 1CK: Investigating the News: Journalism, Technology & the Future\nPSYC 199: Undergraduate Research\nMS&E 472: Entrepreneurial Thought Leaders‚Äô Seminar\n\n\n\n\n\nSYMSYS 1: Minds and Machines\nCS 106A: Programming Methodology\nMath 21: Calculus II\nMATSCI 83N: Great Inventions that Matter\nCS 523: Seminar in Artificial Intelligence in Healthcare\nEDUC 157: Election 2020"
  }
]